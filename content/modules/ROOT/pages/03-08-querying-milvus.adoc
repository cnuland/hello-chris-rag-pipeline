= 3.7 Querying the RAG System
include::_attributes.adoc[]

Now that your pipeline has successfully fetched the ServiceNow data, generated embeddings, and populated the Milvus vector database, you are ready to test the system.

In this final exercise, you will act as an IT support engineer and ask a question in natural language. The system will use the knowledge from the ServiceNow tickets stored in Milvus to provide a helpful, context-aware answer.

== What is Retrieval-Augmented Generation (RAG)?

An LLM is a very capable tool, but its knowledge is limited to the public data it was trained on. It doesn't know about {company-name}'s internal IT procedures, past incident resolutions, or specific server names.

*Retrieval-Augmented Generation (RAG)* solves this problem. Instead of retraining the model, we provide it with relevant, up-to-date information *at the time we ask a question*.

Here's how it works:
1. When you ask a question, we first search our *Milvus vector database* for past incident tickets that are semantically similar to your question.
2. We "retrieve" the most relevant resolutions from those tickets.
3. We then "augment" our prompt to the LLM by including this retrieved context along with your original question.
4. The LLM uses this specific context to generate a precise, helpful answer based on our company's own data.

This technique is powerful because you can continuously update the knowledge base (by running your data ingestion pipeline) without ever having to modify or retrain the LLM itself.

== Querying Milvus from a Notebook

1.  Navigate back to the OpenShift AI dashboard and launch your pre-created *Jupyter Workbench* (named "My Workbench" or similar).

2.  Once JupyterLab has started, use the file browser on the left to navigate to the lab materials folder. The path is:
+
[.console-input]
[source,text]
----
hello-chris-rag-pipeline/lab-contents/3.8
----

3.  Open the notebook named `03-08-retrieval-augmented-generation.ipynb`.

4.  Follow the instructions in the notebook by executing the cells one by one. The notebook will guide you through the following steps.

Press the play button each time to step through each block and run the code

=== Notebook Step 1: Imports and Connecting to Milvus

The first cells import the necessary libraries and set up the connection to the Milvus vector database that your pipeline just populated. It uses LangChain to simplify the process.

```python
# In your notebook...
from langchain.chains import RetrievalQA
from langchain_community.llms import VLLMOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Milvus

# 1. Define which embedding model to use (must match the one used in the pipeline)
embedding_model = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2",
    model_kwargs={'device': 'cpu'}
)

# 2. Connect to the existing Milvus collection, specifying the vector and text fields.
vector_db = Milvus(
    embedding_function=embedding_model,
    connection_args={"host": "vectordb-milvus", "port": "19530"},
    collection_name="servicenow_incidents",
    vector_field="embedding",
    text_field="resolution_notes"
)

# 3. Create a retriever to search for relevant documents
retriever = vector_db.as_retriever(search_kwargs={"k": 3}) # Retrieve top 3 results

print("Successfully connected to Milvus and created a retriever.")
```

=== Notebook Step 2: Defining the LLM and Prompt

Next, the notebook defines the prompt template and configures the connection to the LLM that is being served within OpenShift AI. The template instructs the model on how to behave and where to place the retrieved context.

```python
# In your notebook...
from langchain.prompts import PromptTemplate

# LLM Inference Server URL
# This points to a model served within the cluster.
inference_server_url = "[http://llm-model-server.rag-pipeline-workshop.svc.cluster.local:8080](http://llm-model-server.rag-pipeline-workshop.svc.cluster.local:8080)"

# LLM definition using a client that speaks the OpenAI API format
llm = VLLMOpenAI(
    openai_api_key="EMPTY",
    openai_api_base=f"{inference_server_url}/v1",
    model_name="llm-model", # The name of the deployed model
    temperature=0.1,
    max_tokens=512,
)

# Define the prompt template for our RAG chain
template = """
You are a helpful and professional IT support assistant for Parasol Company.
Use the following context from past incident resolutions to answer the question.
If the context does not contain the answer, say that you don't have enough information.

Context:
{context}

Question:
{question}

Helpful Answer:
"""

prompt = PromptTemplate(template=template, input_variables=["context", "question"])
```

=== Notebook Step 3: Understanding the Two-Step RAG Process

The notebook will now show you the RAG process broken down into two distinct steps to help you understand how the answer is determined.

*Step 1: Retrieval - Finding Relevant Documents*

First, you'll see what happens during the retrieval step by manually searching the vector database:

```python
# In your notebook...
query = "Give me information on INC001004"

# Step 1: RETRIEVAL - Let's see what documents the vector database finds
retrieved_docs = retriever.get_relevant_documents(query)

print(f"Found {len(retrieved_docs)} relevant documents (top-{len(retrieved_docs)}):")

for i, doc in enumerate(retrieved_docs, 1):
    print(f"Document {i}:")
    print(f"   Incident ID: {doc.metadata.get('incident_pk', 'N/A')}")
    print(f"   Description: {doc.metadata.get('short_description', 'N/A')}")
    print(f"   Resolution: {doc.page_content[:200]}...")
```

This step shows you exactly which incident tickets were found to be most similar to your query based on vector similarity search.

*Step 2: Generation - Using Retrieved Context*

Next, you'll see how the LLM uses these retrieved documents as context to generate an answer:

```python
# In your notebook...

# Step 2: GENERATION - Using the retrieved documents to generate an answer
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=True,
)

# Execute the full RAG chain (retrieval + generation)
resp = rag_chain.invoke({"query": query})

# Display the final answer
print("Final Answer from LLM:")
print(resp["result"])
```

The output will show:

1. **The retrieved documents**: The top-k most relevant incident tickets found in the vector database
2. **The final answer**: A concise response generated by the LLM using those documents as context
3. **Clear connection**: How the retrieved documents directly informed the LLM's answer

[.bordershadow]
image::03/rag-query-output.png[Example output from the Jupyter Notebook showing the LLM's answer and the source incident retrieved from Milvus.]

== Understanding What You Just Accomplished

You've now experienced the complete RAG process broken down into its two main components:

1. **üîç Retrieval**: The system searched the Milvus vector database and found the most semantically similar documents to your query using vector embeddings.

2. **ü§ñ Generation**: The LLM used those retrieved documents as context to generate a specific, relevant answer based on your company's actual incident data.

This transparent two-step process demonstrates:

* **How vector similarity works**: You can see exactly which documents were considered most relevant to your query
* **Context-aware generation**: The LLM's answer is grounded in real, specific incident data from your organization
* **Transparency**: You understand exactly where the answer came from and can validate its accuracy
* **Updatable knowledge**: New incidents added to Milvus are immediately available for future queries without retraining the model

For {company-name}, this means support engineers can now ask complex questions and receive answers based on the collective knowledge from thousands of past incident tickets, with full visibility into how those answers were derived.

Congratulations! You have successfully built and tested an end-to-end RAG system on OpenShift AI.
