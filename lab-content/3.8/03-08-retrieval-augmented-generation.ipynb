{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Querying the RAG System with LangChain ##\n",
    "An LLM is a very capable tool, but its knowledge is limited to the public data it was trained on. It doesn't know about Parasol Company's internal IT procedures or the solutions to our past incidents. How can we make it answer questions using our specific, private data?\n",
    "\n",
    "There are a few ways to solve this problem:\n",
    "\n",
    "Full Retraining: This involves re-training the entire model from scratch with our data included. This is incredibly expensive and time-consuming, feasible for only a handful of organizations.\n",
    "\n",
    "Fine-Tuning: We can \"tune\" an existing model on our data. This is much faster and cheaper. It's excellent for teaching the model a specific style, tone, or new skill, but less effective for injecting large amounts of factual knowledge. The model must also be re-tuned whenever the data is updated.\n",
    "\n",
    "Retrieval-Augmented Generation (RAG): This is the technique we will use. We put our private knowledge into an external database (in our case, a `Milvus` vector database) and \"retrieve\" the most relevant pieces of information when a user asks a question. We then feed this retrieved context, along with the original question, to the LLM. The LLM uses this specific context to generate a highly relevant and accurate answer. This is powerful because we can continuously update our knowledge base without ever having to retrain the model.\n",
    "\n",
    "In the previous step, your data science pipeline successfully fetched closed incident tickets from our mock `ServiceNow API`, generated vector embeddings from them, and loaded them into a Milvus database.\n",
    "\n",
    "In this notebook, we will use RAG to ask questions about IT support issues and see how the LLM can provide precise answers based on the historical incident data we just ingested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### Requirements and Imports\n",
    "\n",
    "Import the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment the following line only if you have not selected the right workbench image, or are using this notebook outside of the workshop environment.\n",
    "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt\n",
    "\n",
    "import json\n",
    "import transformers\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Milvus # Use the standard Milvus vector store\n",
    "\n",
    "# Turn off warnings when downloading the embedding model\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain elements\n",
    "\n",
    "Again, we are going to use Langchain to define our task pipeline.\n",
    "\n",
    "First, the **LLM** where we will send our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM Inference Server URL\n",
    "inference_server_url = \"http://granite-3-1-8b-instruct-predictor.shared-llm.svc.cluster.local:8080\"\n",
    "\n",
    "# LLM definition\n",
    "llm = VLLMOpenAI(           # We are using the vLLM OpenAI-compatible API client. But the Model is running on OpenShift AI, not OpenAI.\n",
    "    openai_api_key=\"EMPTY\",   # And that is why we don't need an OpenAI key for this.\n",
    "    openai_api_base= f\"{inference_server_url}/v1\",\n",
    "    model_name=\"granite-3-1-8b-instruct\",\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa13907-14f1-4995-9756-8778c19a2101",
   "metadata": {},
   "source": [
    "Then the connection to the **vector database** where we have prepared the ServiceNow data we pulled from mock API and which was stored in the vector database via the pipeline we deployed and ran in the earlier step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f849c1a0-7fe5-425f-853d-6a9e67a38971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Milvus\n",
    "\n",
    "# 1. Define the embedding model (must match the ingestion pipeline)\n",
    "print(\"Loading embedding model...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    show_progress=False,\n",
    ")\n",
    "print(\"Embedding model loaded.\")\n",
    "\n",
    "# 2. Define connection arguments\n",
    "connection_args = {\n",
    "    \"host\": \"vectordb-milvus\", # The Kubernetes service name for Milvus\n",
    "    \"port\": \"19530\"\n",
    "}\n",
    "print(f\"Connecting to Milvus at: {connection_args['host']}:{connection_args['port']}\")\n",
    "\n",
    "# 3. Connect to the Milvus vector store, specifying the correct text and vector fields\n",
    "vector_db = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args=connection_args,\n",
    "    collection_name=\"servicenow_incidents\",\n",
    "    vector_field=\"embedding\",      # Specify the name of your vector field\n",
    "    text_field=\"resolution_notes\"  # <-- THIS IS THE FIX: Tell LangChain to use this field for page_content\n",
    ")\n",
    "print(\"Successfully connected to Milvus collection 'servicenow_incidents'.\")\n",
    "\n",
    "\n",
    "# 4. Create a retriever to search for relevant documents\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"Retriever created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "We will now define the **template** to use to make our query. Note that this template now contains a **servicenow_incidents** section. That's were the documents returned from the vector database will be injected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "### Step 1: Retrieval - Finding Relevant Documents\n",
    "\n",
    "Let's first understand what happens during the **retrieval** step. We'll manually search the vector database to see what documents are considered most relevant to our query, and then show how these documents are used as context for the LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac009d5-d558-4258-9735-4fb0de46c309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define our query\n",
    "query = \"Give me information on INC001004\"\n",
    "print(f\"🔍 Query: '{query}'\\n\")\n",
    "\n",
    "# Step 1: RETRIEVAL - Let's see what documents the vector database finds\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: RETRIEVAL - Finding relevant documents\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Perform the search manually to show the retrieval step\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"📊 Found {len(retrieved_docs)} relevant documents (top-{len(retrieved_docs)}):\\n\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"📄 Document {i}:\")\n",
    "    print(f\"   Incident ID: {doc.metadata.get('incident_pk', 'N/A')}\")\n",
    "    print(f\"   Description: {doc.metadata.get('short_description', 'N/A')}\")\n",
    "    print(f\"   Resolution: {doc.page_content[:200]}{'...' if len(doc.page_content) > 200 else ''}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n💡 These documents will now be used as context for the LLM to generate an answer.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: Generation - Using Retrieved Context to Generate an Answer\n",
    "# Now let's see how the LLM uses the retrieved documents as context to generate a relevant answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5040b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: GENERATION - Using the retrieved documents to generate an answer\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 2: GENERATION - Creating answer using retrieved context\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Define a prompt template that shows the LLM how to use the retrieved context\n",
    "prompt_template_str = \"\"\"\n",
    "<|system|>\n",
    "You are a helpful, respectful and honest assistant named \"Parasol Assistant\".\n",
    "You will be given context from past incident tickets and a question.\n",
    "Your answer should be based only on the provided context.\n",
    "If the context does not contain the answer, say that you don't have enough information.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template_str, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# 2. Create the RAG chain with the prompt\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "# 3. Execute the full RAG chain (retrieval + generation)\n",
    "print(f\"🤖 Sending query to LLM with retrieved context...\")\n",
    "resp = rag_chain.invoke({\"query\": query})\n",
    "\n",
    "# 4. Display the final answer\n",
    "print(f\"\\n🎯 Final Answer from LLM:\\n\")\n",
    "print(\"-\" * 30)\n",
    "print(resp[\"result\"])\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"\\n✅ This answer was generated using the {len(resp['source_documents'])} documents retrieved above as context.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8cd32-0bdb-484d-a8bd-fb108ce2f131",
   "metadata": {},
   "source": [
    "## Understanding the RAG Process\n",
    "\n",
    "Congratulations! You've now seen the complete RAG process broken down into its two main components:\n",
    "\n",
    "1. **🔍 Retrieval**: The system searched the Milvus vector database and found the most semantically similar documents to your query using vector embeddings.\n",
    "\n",
    "2. **🤖 Generation**: The LLM used those retrieved documents as context to generate a specific, relevant answer based on your company's actual incident data.\n",
    "\n",
    "This two-step process is what makes RAG so powerful - it combines the reasoning capabilities of large language models with the specific, up-to-date knowledge stored in your private databases.\n",
    "\n",
    "## Key Benefits You've Just Demonstrated\n",
    "\n",
    "- **Factual Accuracy**: The LLM's answer is grounded in real incident data, not just its training knowledge\n",
    "- **Transparency**: You can see exactly which documents influenced the answer\n",
    "- **Updatable Knowledge**: Adding new incidents to Milvus immediately makes them available for future queries\n",
    "- **Cost Effective**: No need to retrain expensive models when your data changes\n",
    "\n",
    "For Parasol Company, this means support engineers can now ask complex questions and receive answers based on the collective knowledge from thousands of past incident tickets. This is the foundation of a powerful system that can reduce resolution times, improve support consistency, and accelerate new hire training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
